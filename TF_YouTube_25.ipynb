{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전이 학습(Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:25px\">개념 및 필요성</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:22px\">1. 개념</span>\n",
    "\n",
    "- <span style=\"font-size:18px\">전이 학습이란 아주 큰 데이터 셋, 즉 21,841 부류에 대해서 총 1419만 7122장의 이미지로<br> 구성되어 있는 ImageNet 데이터를 사용해서 학습된 모델의 가중치를 가져와서, 우리가 해<br>결하려는 문제에 맞게 보정해서 사용하는 것을 의미한다</span>\n",
    "\n",
    "- <span style=\"font-size:18px\">이때 큰 데이터 셋을 사용해서 훈련된 모델을 사전 학습 모델(pre-trained model)</span>\n",
    "\n",
    "&emsp;&emsp;<span style=\"font-size:15px\">※ ImageNet 데이터의 이미지 크기는 평균적으로 469×387이며, 이러한 2만 개 이상의 부류 가운데 \n",
    "<br>&emsp;&emsp;&emsp;1000 부류만 뽑아서 데이터를 구성하고, 정확도를 높이는 대화가 바로 ImageNet Challange</span>\n",
    "\n",
    "<span style=\"font-size:22px\">2. 필요성</span>\n",
    "\n",
    "- <span style=\"font-size:18px\">CNN 모델의 품질을 높이기 위해서, 즉 임의의 데이터에 대해서 정확도는 높이고 overfitting은 줄이기 위해서 기본적으로 많은 양의 데이터를 이용하여 학습하여야 한다</span>\n",
    "\n",
    "- <span style=\"font-size:18px\">그러나 많은 학습 데이터를 확보하려면 시간과 비용이 많이 소요되기에, 이 부분을 해결하고자 등장한 것이 전이 학습이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:25px\">사전 학습 모델(pre-trained model) 구조</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"font-size:17px\">\n",
    "▶ 사전 학습된 특징 추출기<br>\n",
    "\n",
    "- 특징 추출기(feature extractor)는 convolution <br>layer과 pooling layer의 조합으로 구성되어 있<br>으며 ImageNet 데이터에 대해 이미 학습되어 있음\n",
    "</td>\n",
    "<td style=\"font-size:17px\">\n",
    "<span style=\"font-size:20px\">Conv ⇒ Conv ⇒ Pooling ⇒</span>\n",
    "\n",
    "※ 특징 추출기의 출력 데이터를 bottleneck 또는 <br>feature vector 등으로 지정함\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"font-size:17px\">\n",
    "▶ 사전 학습된 분류기<br>\n",
    "\n",
    "- 분류기(classifier)는 fully-connected layer으로 <br>구성되며 추출된 특징을 입력 받아 최종 주어<br>지는 이미지에 대한 클래스(정답)을 카테고리 <br>형태로 분류하는 역할 수행\n",
    "</td>\n",
    "<td style=\"font-size:17px\">\n",
    "<span style=\"font-size:20px\">Dense ⇒ Dense ⇒ Dense(softmax) ⇒</span>\n",
    "\n",
    "※ overfitting을 줄이기 위해 output layer 이전의 <br>Dense layer 사이에는 Dropout, BatchNormalization <br>layer 등을 add 할 수 있음\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:25px\">사전 학습 모델(pre-trained model) 종류</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">TensorFlow가 제공하는 pre-trained model 이름과 크기, ImageNet 데이터에 대한<br> 1순위와 5순위 정확도, 파라미터 개수 및 층의 깊이 등을 나타낸다</span>\n",
    "\n",
    "- <span style=\"font-size:17px\">사용할 데이터 셋에 따라 모델의 성능이 다르기 때문에 모델 간의 비교가 중요</span>\n",
    "\n",
    "<img src=\"picture/TF_img_25_1.png\" alt=\"TF_img_25_1\" width=850>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:25px\">파인 튜닝(fine-tuning)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">pre-trained model의 가중치를 미세하게 조정하는 기법\n",
    "<br>new data의 일부 혹은 전체를 미리 분석한 후, 이를 베이스로 pre-trained model 가중치 일부 또는 전체 가중치를 재학습 하는 것이다</span>\n",
    "\n",
    "- <span style=\"font-size:17px\">fine-tuning 시 많은 연산량이 필요하기에 GPU를 사용하는 것이 좋다</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:25px\">Transfer Learning 사용법</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 6s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 240, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 240, 240, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 240, 240, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 120, 120, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 120, 120, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 120, 120, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 60, 60, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 60, 60, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 60, 60, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 60, 60, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 30, 30, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNet, InceptionV3\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(240, 240, 3))\n",
    "#                 1. 사전학습에 사용된 데이터 셋\n",
    "#                                     2. False 사전 학습 모델의 특징추출기만 가져옴\n",
    "#                                        True 특징추출기와 분류기 모두 가져옴\n",
    "#                                                         3. 새롭게 학습할 이미지 텐서 크기\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                1605696   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,320,644\n",
      "Trainable params: 16,320,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 이전 학습된 내용 추출\n",
    "model.add(base_model)\n",
    "model.add(Flatten())    # Flatten() or GlobalAveragePooling2D()\n",
    "                        # 변환\n",
    "# 새로운 데이터 학습 및 이전 학습과 새로운 학습을 연결\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
